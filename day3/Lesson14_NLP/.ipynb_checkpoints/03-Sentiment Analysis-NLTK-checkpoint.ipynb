{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with NTLK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data Source, and Prepare Training and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, FreqDist\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the labelled positive and negative text\n",
    "def DataSources():\n",
    "    positiveData = open(\"resources/positive.txt\", \"r\", encoding='utf-8', errors='replace').read()\n",
    "    negativeData = open(\"resources/negative.txt\", \"r\", encoding='utf-8', errors='replace').read()\n",
    "    return positiveData, negativeData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "def PrepareData():\n",
    "    train_pos, train_neg = DataSources()\n",
    "    documents = []\n",
    "    all_words = []\n",
    "    \n",
    "#    j is adjective, r is adverb, and v is verb\n",
    "#    allowed_word_types = [\"J\",\"R\",\"V\"]\n",
    "    allowed_word_types = [\"J\"]\n",
    "\n",
    "    for p in train_pos.split('\\n'):\n",
    "        documents.append((p, \"pos\"))\n",
    "        words = word_tokenize(p)\n",
    "        pos = pos_tag(words)\n",
    "        for w in pos:\n",
    "            if w[1][0] in allowed_word_types:\n",
    "                all_words.append(w[0].lower())\n",
    "\n",
    "    for p in train_neg.split('\\n'):\n",
    "        documents.append((p, \"neg\"))\n",
    "        words = word_tokenize(p)\n",
    "        pos = pos_tag(words)\n",
    "        for w in pos:\n",
    "            if w[1][0] in allowed_word_types:\n",
    "                all_words.append(w[0].lower())\n",
    "    \n",
    "    save_documents = open(\"saved/documents.p\", \"wb\")\n",
    "    pickle.dump(documents, save_documents)\n",
    "    save_documents.close()\n",
    "\n",
    "    all_words = FreqDist(all_words)\n",
    "    word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "    save_word_features = open(\"saved/word_features5k.p\", \"wb\")\n",
    "    pickle.dump(word_features, save_word_features)\n",
    "    save_word_features.close()\n",
    "\n",
    "    features = [(find_features(rev, word_features), category) for (rev, category) in documents]\n",
    "    return features\n",
    "\n",
    "\n",
    "def find_features(document, word_features):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "\n",
    "def TestTrainData():\n",
    "    featuresets = PrepareData()\n",
    "    random.shuffle(featuresets)\n",
    "#    print(len(featuresets))\n",
    "    testing_set = featuresets[10000:]\n",
    "    training_set = featuresets[:10000]\n",
    "    return training_set, testing_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifiers and store them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk import NaiveBayesClassifier, classify\n",
    "# from gsu.data import TestTrainData\n",
    "import pickle\n",
    "\n",
    "\n",
    "def TrainClassifiers():\n",
    "    training_set, testing_set = TestTrainData()\n",
    "\n",
    "    classifiers = list()\n",
    "    classifier_name = list()\n",
    "\n",
    "    NaiveBayesClassifier_classifier = NaiveBayesClassifier.train(training_set)\n",
    "    classifiers.append(NaiveBayesClassifier_classifier)\n",
    "    classifier_name.append(\"NaiveBayesClassifier\")\n",
    "\n",
    "    MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "    MNB_classifier.train(training_set)\n",
    "    classifiers.append(MNB_classifier)\n",
    "    classifier_name.append(\"MultinomialNBClassifier\")\n",
    "\n",
    "    BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "    BernoulliNB_classifier.train(training_set)\n",
    "    classifiers.append(BernoulliNB_classifier)\n",
    "    classifier_name.append(\"BernoulliNBClassifier\")\n",
    "\n",
    "    LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "    LogisticRegression_classifier.train(training_set)\n",
    "    classifiers.append(LogisticRegression_classifier)\n",
    "    classifier_name.append(\"LogisticRegressionClassifier\")\n",
    "\n",
    "    LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "    LinearSVC_classifier.train(training_set)\n",
    "    classifiers.append(LogisticRegression_classifier)\n",
    "    classifier_name.append(\"LinearSVCClassifier\")\n",
    "\n",
    "    SGDC_classifier = SklearnClassifier(SGDClassifier())\n",
    "    SGDC_classifier.train(training_set)\n",
    "    classifiers.append(SGDC_classifier)\n",
    "    classifier_name.append(\"SGDClassifier\")\n",
    "\n",
    "    SaveClassifiers(classifiers, classifier_name)\n",
    "\n",
    "    return classifiers\n",
    "\n",
    "\n",
    "def SaveClassifiers(classifiers, classifier_name):\n",
    "\n",
    "    for i in range(0, len(classifiers)):\n",
    "        save_classifier_path = open(\"saved/\" + classifier_name[i] + \".p\", \"wb\")\n",
    "        pickle.dump(classifiers[i], save_classifier_path)\n",
    "        save_classifier_path.close()\n",
    "\n",
    "    save_classifier_path = open(\"saved/classifier_name.p\", \"wb\")\n",
    "    pickle.dump(classifier_name, save_classifier_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import classify\n",
    "\n",
    "\n",
    "\n",
    "def find_features(document, word_features):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "\n",
    "def LoadData(shuffle=False):\n",
    "\n",
    "    documents_f = open(\"saved/documents.p\", \"rb\")\n",
    "    documents = pickle.load(documents_f)\n",
    "    documents_f.close()\n",
    "\n",
    "    documents_f = open(\"saved/word_features5k.p\", \"rb\")\n",
    "    word_features = pickle.load(documents_f)\n",
    "    documents_f.close()\n",
    "\n",
    "    features = [(find_features(rev, word_features), category) for (rev, category) in documents]\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(features)\n",
    "\n",
    "    testing_set = features[10000:]\n",
    "    training_set = features[:10000]\n",
    "\n",
    "    return training_set, testing_set\n",
    "\n",
    "\n",
    "def LoadClassifiers():\n",
    "    document = open(\"saved/classifier_name.p\", \"rb\")\n",
    "    classifier_name = pickle.load(document)\n",
    "    document.close()\n",
    "    # print(classifier_name)\n",
    "    classifiers = list()\n",
    "\n",
    "#     training_set, testing_set = LoadData()\n",
    "\n",
    "    for name in classifier_name:\n",
    "        document = open(\"saved/\" + name + \".p\", \"rb\")\n",
    "        classifier = pickle.load(document)\n",
    "        classifiers.append(classifier)\n",
    "        document.close()\n",
    "\n",
    "    return classifiers\n",
    "\n",
    "\n",
    "def LoadFeatures():\n",
    "    documents_f = open(\"saved/word_features5k.p\", \"rb\")\n",
    "    word_features = pickle.load(documents_f)\n",
    "    documents_f.close()\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode, StatisticsError\n",
    "\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        ret = \"neg\"\n",
    "        try:\n",
    "            ret = mode(votes)\n",
    "        except StatisticsError:\n",
    "            # print(\"Caught1\")\n",
    "            pass\n",
    "        return ret\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        try:\n",
    "            choice_votes = votes.count(mode(votes))\n",
    "            conf = choice_votes / len(votes)\n",
    "            return conf\n",
    "        except StatisticsError:\n",
    "            # print(\"Caught2\")\n",
    "            return 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classifier : Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from gsu.train import TrainClassifiers\n",
    "# from gsu.load import LoadClassifiers\n",
    "# from gsu.load import LoadFeatures\n",
    "# from gsu.data import find_features\n",
    "# from gsu.VoteClassifier import VoteClassifier\n",
    "\n",
    "\n",
    "class Sentiment:\n",
    "\n",
    "    def __init__(self):\n",
    "        classifiers = TrainClassifiers()\n",
    "#         classifiers = LoadClassifiers()\n",
    "        self.votedClassifier = VoteClassifier(classifiers)\n",
    "        self.new_features = LoadFeatures()\n",
    "\n",
    "    def Analyse(self, text):\n",
    "        new_features = find_features(text, self.new_features)\n",
    "        return self.votedClassifier.classify(new_features), self.votedClassifier.confidence(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for positive or negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off pickle warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved/classifier_name.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-fee540431659>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# from gsu.Sentiment import Sentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAnalyse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"This movie was awesome! The acting was great, plot was wonderful!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-4ddb7e78c310>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#         classifiers = TrainClassifiers()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mclassifiers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoadClassifiers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvotedClassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVoteClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoadFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c19bea4dee14>\u001b[0m in \u001b[0;36mLoadClassifiers\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mLoadClassifiers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saved/classifier_name.p\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mclassifier_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved/classifier_name.p'"
     ]
    }
   ],
   "source": [
    "# from gsu.Sentiment import Sentiment\n",
    "\n",
    "s = Sentiment()\n",
    "\n",
    "print(s.Analyse(\"This movie was awesome! The acting was great, plot was wonderful!\"))\n",
    "\n",
    "print(s.Analyse(\"I am happy and awesome\"))\n",
    "\n",
    "print(s.Analyse(\"This movie was boring\"))\n",
    "\n",
    "print(s.Analyse(\"Evrything is bad. Movie sucks!\"))\n",
    "\n",
    "print(s.Analyse(\"Amazing movie, my kids love it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('neg', 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(s.Analyse(\"lousy acting\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('neg', 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(s.Analyse(\"go to hell\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
